\documentclass[10pt,letterpaper]{article}
\input{config/config.tex}
\makeglossaries

\usepackage{paracol}

\begin{document}

\setcounter{page}{1}
\pagestyle{fancy}
\setlength{\headheight}{15pt}
\fancyhead[L]{ML Exercise 1}
\fancyhead[R]{Group 05: 12229237, 12512103, 01608730}  

\newpage    

\begin{center}
    \section*{\LARGE Machine Learning Exercise 1\\Classification
}
\end{center}

%\input{info_datasets}

\section{Datasets}

\subsection{Accident Dataset}

This dataset has been described in Exercise 0. Please refer to the description there.

\subsection{Breast Cancer Dataset}

asdgasdrfg

\subsection{Hotel Reservations Dataset}

\subsection{Loan Dataset}

\section{Experimental Setup}

\subsection{Environment and Libraries}

We used Python for the implementation as the main development tool. For implementing the algorithms, we used libraries from the sklearn framework, whereas for data manipulation, we used numpy and pandas. For the visualization, matplotlib has been the main tool.

\subsection{Preprocessing}

\section{Chosen Classifiers}

\subsection{K Nearest Neighbors}

Since this classifier is one of the most widely known and used classifiers, we decided to use the k-NN classifier. It has no training step, but computes the class prediction at runtime. The hyperparameter, we consider here, is k. After having tried different vlaues for the distance function, we saw that this hyperparameter did not have an important impact on the results.

The k-NN classifier is easy to understand and interpret, but it is very sensitive to noise. However, larger values of k reduce the effect of noise. Another drawback of this classifier is that in case there are many features, its performance can fall drastically.

\subsection{Logistic Regression}

\subsection{Random Forest}

\section{Evaluation Methodology}

\subsection{Performance Measures}

For each classifier, we use the same evaluation metrics to be able to compare them. We are using the following measures:

\begin{itemize}
    \item Balanced Accuracy: The accuracy is most commonly used, because it measures the overall percentage of correct predictions. We use the balanced version, because it gives equal importance to each class, making it fairer for imbalanced datasets.
    \item F1 Score: Combines Precision and Recall. This metric is also fairly considering imbalanced classes.
    \item ROC-AUC: TO BE DONE 
    \item Fit time: How long it took to fit
    \item Score time: How long it took to make predictions
\end{itemize}

\subsection{Holdout vs. Cross-Validation}

\section{Results}

\subsection{Accident Dataset}

\subsubsection{k-NN Classifier}

\subsubsection{Logistic Regression  Classifier}

\subsubsection{Random Forest  Classifier}

\subsection{Breast Cancer Dataset}

\subsubsection{k-NN Classifier}

\subsubsection{Logistic Regression Classifier}

\subsubsection{Random Forest Classifier}

\subsection{Hotel Reservations Dataset}

\subsubsection{k-NN Classifier}

\subsubsection{Logistic Regression Classifier}

\subsubsection{Random Forest Classifier}

\subsection{Loan Dataset}

\subsubsection{k-NN Classifier Classifier}

\subsubsection{Logistic Regression Classifier}

\subsubsection{Random Forest Classifier}

\section{Conclusion}

\newpage
\printbibliography
\end{document}
